import requests
import random
import re
import json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import markdown
import feedparser
import time
import urllib.parse
import hashlib
import base64
import sys
import os

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from interactive_features import interactive_features
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥äº’åŠ¨åŠŸèƒ½æ¨¡å—")
    interactive_features = None

try:
    from sentiment_analyzer import sentiment_analyzer
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥æƒ…æ„Ÿåˆ†ææ¨¡å—")
    sentiment_analyzer = None

try:
    from html_generator import html_generator
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥HTMLç”Ÿæˆæ¨¡å—")
    html_generator = None

try:
    from api_generator import api_generator
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥APIç”Ÿæˆæ¨¡å—")
    api_generator = None

try:
    from rss_generator import rss_generator
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥RSSç”Ÿæˆæ¨¡å—")
    rss_generator = None

def get_programming_quote():
    """ä»ç¼–ç¨‹è¯­å½•APIè·å–åè¨€"""
    try:
        response = requests.get("https://programming-quotes-api.herokuapp.com/Quotes/random")
        if response.status_code == 200:
            data = response.json()
            return f"{data['en']} â€”â€” {data['author']}"
    except:
        pass
    
    # å¤‡ç”¨åè¨€åˆ—è¡¨
    quotes = [
        "ç®€å•æ˜¯å¯é çš„å…ˆå†³æ¡ä»¶ã€‚ â€”â€” Edsger W. Dijkstra",
        "è½¯ä»¶å°±åƒåšçˆ±ï¼Œä¸€æ¬¡çŠ¯é”™ï¼Œä½ éœ€è¦ç”¨ä½™ç”Ÿæ¥ç»´æŠ¤ã€‚ â€”â€” Michael Sinz",
        "ä»»ä½•å‚»ç“œéƒ½èƒ½å†™å‡ºè®¡ç®—æœºèƒ½ç†è§£çš„ä»£ç ã€‚ä¼˜ç§€çš„ç¨‹åºå‘˜èƒ½å†™å‡ºäººèƒ½ç†è§£çš„ä»£ç ã€‚ â€”â€” Martin Fowler",
        "è°ƒè¯•ä»£ç æ¯”å†™ä»£ç éš¾ä¸¤å€ã€‚å› æ­¤ï¼Œå¦‚æœä½ å†™ä»£ç æ—¶å°½å¯èƒ½èªæ˜ï¼Œé‚£ä¹ˆä½ åœ¨è°ƒè¯•æ—¶ä¼šæ˜¾å¾—ä¸å¤Ÿèªæ˜ã€‚ â€”â€” Brian W. Kernighan",
        "å…ˆè®©å®ƒå·¥ä½œï¼Œå†è®©å®ƒæ­£ç¡®ï¼Œæœ€åè®©å®ƒå¿«é€Ÿå·¥ä½œã€‚ â€”â€” Kent Beck",
        "ç¼–ç¨‹ä¸æ˜¯å…³äºä½ çŸ¥é“ä»€ä¹ˆï¼Œè€Œæ˜¯å…³äºä½ èƒ½è§£å†³ä»€ä¹ˆé—®é¢˜ã€‚ â€”â€” V. Anton Spraul",
        "ä»£ç æ˜¯å†™ç»™äººçœ‹çš„ï¼Œåªæ˜¯é¡ºä¾¿èƒ½åœ¨æœºå™¨ä¸Šè¿è¡Œã€‚ â€”â€” Harold Abelson",
        "æœ€å¥½çš„ç¨‹åºå‘˜ä¸ä»…æ˜¯ç¼–ç¨‹é«˜æ‰‹ï¼Œè¿˜çŸ¥é“å“ªäº›ä»£ç ä¸éœ€è¦å†™ã€‚ â€”â€” Bill Gates",
        "ç¼–ç¨‹çš„è‰ºæœ¯å°±æ˜¯å¤„ç†å¤æ‚æ€§çš„è‰ºæœ¯ã€‚ â€”â€” Edsger W. Dijkstra",
        "è½¯ä»¶è®¾è®¡çš„ç›®æ ‡æ˜¯æ§åˆ¶å¤æ‚æ€§ï¼Œè€Œä¸æ˜¯å¢åŠ å¤æ‚æ€§ã€‚ â€”â€” Pamela Zave"
    ]
    return random.choice(quotes)

def get_github_trending(language=None, since="daily"):
    """è·å–GitHubè¶‹åŠ¿é¡¹ç›®"""
    url = "https://github.com/trending"
    if language:
        url += f"/{language}"
    url += f"?since={since}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        trending_repos = []
        repo_list = soup.select('article.Box-row')
        
        for repo in repo_list[:5]:  # åªè·å–å‰5ä¸ª
            name_element = repo.select_one('h2 a')
            if name_element:
                repo_name = name_element.text.strip().replace('\n', '').replace(' ', '')
                repo_url = f"https://github.com{name_element['href']}"
                
                description_element = repo.select_one('p')
                description = description_element.text.strip() if description_element else "No description"
                
                trending_repos.append({
                    'name': repo_name,
                    'url': repo_url,
                    'description': description
                })
        
        return trending_repos
    except Exception as e:
        print(f"è·å–GitHubè¶‹åŠ¿é¡¹ç›®å¤±è´¥: {e}")
        return []

def get_ai_news_from_rss():
    """ä»RSSè®¢é˜…æºè·å–AIç›¸å…³æ–°é—»"""
    # ä¼˜å…ˆä½¿ç”¨CSDNçš„AIèµ„è®¯æº
    primary_source = "https://api.dbot.pp.ua/v1/rss/csdn/ai"
    
    # å®šä¹‰headerså˜é‡
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    try:
        print(f"å°è¯•è·å– AI æ–°é—» RSS: {primary_source}")
        response = requests.get(primary_source, headers=headers, timeout=15)
        print(f"RSS å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            print(f"RSS å“åº”å†…å®¹: {response.text[:500]}...")
            
            # è§£æAtomæ ¼å¼
            feed = feedparser.parse(response.text)
            
            if feed.entries:
                ai_news = []
                print(f"æˆåŠŸè·å– {len(feed.entries)} æ¡ AI æ–°é—»")
                
                for entry in feed.entries[:20]:  # æœ€å¤šè·å–20æ¡
                    title = entry.title
                    url = entry.link if isinstance(entry.link, str) else entry.link[0]['href']
                    
                    # è·å–æè¿°
                    description = ""
                    if hasattr(entry, "summary"):
                        soup = BeautifulSoup(entry.summary, "html.parser")
                        description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                    
                    if not description:
                        description = "AIæŠ€æœ¯åŠ¨æ€ï¼Œè¯¦æƒ…è¯·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹"
                    
                    print(f"æ·»åŠ æ–‡ç« : {title} - URL: {url}")
                    ai_news.append({
                        "title": title,
                        "url": url,
                        "description": description
                    })
                    
                    if len(ai_news) >= 5:  # åªå–å‰5æ¡
                        break
                
                if ai_news:
                    print(f"æœ€ç»ˆè·å–åˆ° {len(ai_news)} æ¡ AI æ–°é—»")
                    for news in ai_news:
                        print(f"- {news['title']} ({news['url']})")
                    return ai_news
    except Exception as e:
        print(f"è·å–RSSæº {primary_source} å¤±è´¥: {e}")
    
    # å¦‚æœä¸»æºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–°é—»
    print("ä½¿ç”¨å¤‡ç”¨ AI æ–°é—»å†…å®¹")
    ai_news = [
        {
            "title": "OpenAIå‘å¸ƒGPT-4 Turboï¼Œæ€§èƒ½å¤§å¹…æå‡",
            "url": "https://openai.com/blog/",
            "description": "æ–°æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡çª—å£æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›"
        },
        # å¯ä»¥æ·»åŠ æ›´å¤šå¤‡ç”¨æ–°é—»
    ]
    
    return ai_news

def get_cybersecurity_news_from_rss():
    """ä»RSSè®¢é˜…æºè·å–ç½‘ç»œå®‰å…¨æ–°é—»"""
    # ç½‘ç»œå®‰å…¨ç›¸å…³RSSè®¢é˜…æºåˆ—è¡¨ - æ›´æ–°ä¸ºæ›´å¯é çš„æº
    rss_feeds = [
        "https://www.freebuf.com/feed",
        "https://api.anquanke.com/data/v1/rss",
        "https://paper.seebug.org/rss",
        "https://www.4hou.com/feed",
        "https://xlab.tencent.com/cn/atom.xml",
        "https://www.sec-wiki.com/news/rss",
        "http://blog.knownsec.com/feed",
        "https://vipread.com/feed",
        "https://www.aqniu.com/feed",
        "https://keenlab.tencent.com/zh/atom.xml",
        "https://blog.netlab.360.com/rss",
        "https://www.seebug.org/rss/new",
        "https://www.secpulse.com/feed",
        "https://tttang.com/rss.xml"
    ]
    
    security_news = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    for feed_url in rss_feeds:
        try:
            print(f"å°è¯•è·å–ç½‘ç»œå®‰å…¨æ–°é—» RSS: {feed_url}")
            response = requests.get(feed_url, headers=headers, timeout=15)
            print(f"RSS å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                print(f"RSS å“åº”å†…å®¹: {response.text[:200]}...")
                continue
                
            print(f"RSS å“åº”å†…å®¹: {response.text[:200]}...")
            
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                print(f"RSS æºæ²¡æœ‰æ¡ç›®: {feed_url}")
                continue
                
            print(f"æˆåŠŸè·å– {len(feed.entries)} æ¡ç½‘ç»œå®‰å…¨æ–°é—»")
            
            for entry in feed.entries[:1]:  # æ¯ä¸ªæºå–å‰1æ¡
                if len(security_news) >= 3:  # æœ€å¤šè·å–3æ¡æ–°é—»
                    break
                    
                title = entry.title
                
                # å¤„ç† Atom æ ¼å¼çš„é“¾æ¥
                if hasattr(entry, "link") and not entry.link.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    url = entry.link
                elif hasattr(entry, "links") and entry.links:
                    # å°è¯•æ‰¾åˆ°éå›¾ç‰‡é“¾æ¥
                    for link in entry.links:
                        if hasattr(link, "href") and not link.href.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                            url = link.href
                            break
                    else:
                        # å¦‚æœæ‰€æœ‰é“¾æ¥éƒ½æ˜¯å›¾ç‰‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé“¾æ¥
                        url = entry.links[0].href
                else:
                    # å°è¯•ä» id å­—æ®µè·å–é“¾æ¥
                    url = entry.id if hasattr(entry, "id") else "#"
                
                # æ£€æŸ¥ URL æ˜¯å¦ä¸ºå›¾ç‰‡é“¾æ¥
                if url.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    print(f"è·³è¿‡å›¾ç‰‡é“¾æ¥: {url}")
                    continue
                
                # å°è¯•è·å–æè¿°
                description = ""
                if hasattr(entry, "summary"):
                    soup = BeautifulSoup(entry.summary, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                elif hasattr(entry, "description"):
                    soup = BeautifulSoup(entry.description, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                elif hasattr(entry, "content") and entry.content:
                    soup = BeautifulSoup(entry.content[0].value, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                
                if not description:
                    description = "å®‰å…¨å…¬å‘Šï¼Œè¯¦æƒ…è¯·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´å†…å®¹"
                
                print(f"æ·»åŠ ç½‘ç»œå®‰å…¨æ–‡ç« : {title} - URL: {url}")
                
                security_news.append({
                    "title": title,
                    "url": url,
                    "description": description
                })
                
            if len(security_news) >= 3:
                break
                
            time.sleep(2)
            
        except Exception as e:
            print(f"è·å–RSSæº {feed_url} å¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            if isinstance(e, requests.exceptions.RequestException):
                print(f"è¯·æ±‚è¯¦æƒ…: {e.request.method} {e.request.url}")
                if hasattr(e, 'response') and e.response:
                    print(f"å“åº”çŠ¶æ€ç : {e.response.status_code}")
                    print(f"å“åº”å¤´: {e.response.headers}")
            continue
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œæ·»åŠ ç½‘ç»œå®‰å…¨æç¤º
    if len(security_news) == 0:
        tips = [
            "å®šæœŸæ›´æ–°æ‰€æœ‰è½¯ä»¶å’Œæ“ä½œç³»ç»Ÿï¼Œä»¥ä¿®è¡¥å·²çŸ¥çš„å®‰å…¨æ¼æ´ã€‚",
            "ä½¿ç”¨å¯†ç ç®¡ç†å™¨ç”Ÿæˆå’Œå­˜å‚¨å¼ºå¯†ç ï¼Œé¿å…åœ¨å¤šä¸ªç½‘ç«™ä½¿ç”¨ç›¸åŒå¯†ç ã€‚",
            "å¯ç”¨åŒå› ç´ è®¤è¯(2FA)ï¼Œä¸ºè´¦æˆ·æ·»åŠ é¢å¤–çš„å®‰å…¨å±‚ã€‚",
            "å®šæœŸå¤‡ä»½é‡è¦æ•°æ®ï¼Œå¹¶éµå¾ª3-2-1å¤‡ä»½è§„åˆ™ï¼š3ä»½æ•°æ®å‰¯æœ¬ï¼Œ2ç§ä¸åŒçš„å­˜å‚¨ä»‹è´¨ï¼Œ1ä»½å¼‚åœ°å­˜å‚¨ã€‚",
            "ä½¿ç”¨VPNä¿æŠ¤å…¬å…±Wi-Fiè¿æ¥æ—¶çš„ç½‘ç»œæµé‡ã€‚"
        ]
        
        for tip in tips[:3]:
            security_news.append({
                "title": "ç½‘ç»œå®‰å…¨æç¤º",
                "url": "https://www.cisa.gov/cybersecurity",
                "description": tip
            })
    
    return security_news[:3]  # è¿”å›æœ€å¤š3æ¡æ–°é—»

def get_tech_job_trends():
    """è·å–æŠ€æœ¯å°±ä¸šè¶‹åŠ¿"""
    trends = [
        "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆéœ€æ±‚æŒç»­å¢é•¿ï¼Œå°¤å…¶æ˜¯å…·æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ç»éªŒçš„ä¸“ä¸šäººæ‰ã€‚",
        "äº‘è®¡ç®—ä¸“å®¶ä»ç„¶æ˜¯å°±ä¸šå¸‚åœºçš„çƒ­é—¨ï¼ŒAWSã€Azureå’ŒGCPè®¤è¯ä»·å€¼æ˜¾è‘—ã€‚",
        "ç½‘ç»œå®‰å…¨äººæ‰ç¼ºå£æ‰©å¤§ï¼Œé›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹ä¸“å®¶éœ€æ±‚æ¿€å¢ã€‚",
        "æ•°æ®ç§‘å­¦å’Œåˆ†æè§’è‰²æŒç»­çƒ­é—¨ï¼Œç‰¹åˆ«æ˜¯èƒ½å¤Ÿå°†æ•°æ®æ´å¯Ÿè½¬åŒ–ä¸ºä¸šåŠ¡ä»·å€¼çš„ä¸“ä¸šäººå£«ã€‚",
        "DevOpså’ŒSREå·¥ç¨‹å¸ˆéœ€æ±‚ç¨³å®šå¢é•¿ï¼Œè‡ªåŠ¨åŒ–å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æŠ€èƒ½å¤‡å—é’çã€‚",
        "å…¨æ ˆå¼€å‘è€…ä»ç„¶æ˜¯å¸‚åœºä¸»åŠ›ï¼ŒReactã€Node.jså’ŒPythonæŠ€èƒ½ç»„åˆç‰¹åˆ«å—æ¬¢è¿ã€‚",
        "åŒºå—é“¾å’ŒWeb3å¼€å‘è€…è™½ç»å†å¸‚åœºæ³¢åŠ¨ï¼Œä½†åœ¨é‡‘èç§‘æŠ€é¢†åŸŸä»æœ‰ç¨³å®šéœ€æ±‚ã€‚",
        "è¿œç¨‹å·¥ä½œæœºä¼šæŒç»­å¢åŠ ï¼Œä½†æ··åˆå·¥ä½œæ¨¡å¼æˆä¸ºè®¸å¤šç§‘æŠ€å…¬å¸çš„æ–°æ ‡å‡†ã€‚",
        "è½¯æŠ€èƒ½å¦‚æ²Ÿé€šã€å›¢é˜Ÿåä½œå’Œé—®é¢˜è§£å†³èƒ½åŠ›åœ¨æŠ€æœ¯æ‹›è˜ä¸­çš„é‡è¦æ€§æ—¥ç›Šæå‡ã€‚",
        "é‡å­è®¡ç®—ä¸“å®¶è™½ç„¶æ˜¯å°ä¼—é¢†åŸŸï¼Œä½†è–ªèµ„æ°´å¹³å’Œå¢é•¿æ½œåŠ›æ˜¾è‘—ã€‚",
        "è¾¹ç¼˜è®¡ç®—å’ŒIoTä¸“å®¶åœ¨åˆ¶é€ ä¸šå’Œæ™ºèƒ½åŸå¸‚é¡¹ç›®ä¸­éœ€æ±‚å¢åŠ ã€‚",
        "å…·å¤‡å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›å’Œè·¨å¹³å°å¼€å‘ç»éªŒçš„å·¥ç¨‹å¸ˆæ›´å…·ç«äº‰åŠ›ã€‚",
        "æ•æ·å’ŒScrumè®¤è¯åœ¨é¡¹ç›®ç®¡ç†è§’è‰²ä¸­ä»·å€¼æå‡ã€‚",
        "ä½ä»£ç /æ— ä»£ç å¹³å°ä¸“å®¶éœ€æ±‚å¢é•¿ï¼Œå°¤å…¶åœ¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹é¡¹ç›®ä¸­ã€‚",
        "å¯æŒç»­æŠ€æœ¯å’Œç»¿è‰²ITä¸“å®¶åœ¨ç¯ä¿æ„è¯†å¢å¼ºçš„ä¼ä¸šä¸­æœºä¼šå¢å¤šã€‚"
    ]
    return random.choice(trends)

def get_tech_news_from_rss():
    """ä»RSSè®¢é˜…æºè·å–ç§‘æŠ€æ–°é—»"""
    # ç§‘æŠ€æ–°é—»ç›¸å…³RSSè®¢é˜…æºåˆ—è¡¨
    rss_feeds = [
        "https://api.dbot.pp.ua/v1/rss/tencent_cloud",
        "https://api.dbot.pp.ua/v1/rss/cnbeta",  # æ·»åŠ è¿™ä¸ªå¯èƒ½æ›´å¯é çš„æº
        "https://rsshub.app/36kr/technology", # 36æ°ªç§‘æŠ€é¢‘é“
        "https://rsshub.app/ifanr/app", # çˆ±èŒƒå„¿
        "https://rsshub.app/sspai/matrix", # å°‘æ•°æ´¾Matrix
        "https://rsshub.app/cnbeta", # cnBeta
        "https://rsshub.app/geekpark/breakingnews" # æå®¢å…¬å›­
    ]
    
    tech_news = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    for feed_url in rss_feeds:
        try:
            print(f"å°è¯•è·å–ç§‘æŠ€æ–°é—» RSS: {feed_url}")
            response = requests.get(feed_url, headers=headers, timeout=15)
            print(f"RSS å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                print(f"RSS å“åº”å†…å®¹: {response.text[:200]}...")
                continue
                
            print(f"RSS å“åº”å†…å®¹: {response.text[:200]}...")
            
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                print(f"RSS æºæ²¡æœ‰æ¡ç›®: {feed_url}")
                continue
                
            print(f"æˆåŠŸè·å– {len(feed.entries)} æ¡ç§‘æŠ€æ–°é—»")
            
            for entry in feed.entries[:3]:  # ç›´æ¥è·å–å‰3æ¡
                title = entry.title
                
                # å¤„ç† Atom æ ¼å¼çš„é“¾æ¥
                if hasattr(entry, "link") and not entry.link.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    url = entry.link
                elif hasattr(entry, "links") and entry.links:
                    # å°è¯•æ‰¾åˆ°éå›¾ç‰‡é“¾æ¥
                    for link in entry.links:
                        if hasattr(link, "href") and not link.href.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                            url = link.href
                            break
                    else:
                        # å¦‚æœæ‰€æœ‰é“¾æ¥éƒ½æ˜¯å›¾ç‰‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé“¾æ¥
                        url = entry.links[0].href
                else:
                    # å°è¯•ä» id å­—æ®µè·å–é“¾æ¥
                    url = entry.id if hasattr(entry, "id") else "#"
                
                # æ£€æŸ¥ URL æ˜¯å¦ä¸ºå›¾ç‰‡é“¾æ¥
                if url.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    print(f"è·³è¿‡å›¾ç‰‡é“¾æ¥: {url}")
                    continue
                
                # å°è¯•è·å–æè¿°
                description = ""
                if hasattr(entry, "summary"):
                    soup = BeautifulSoup(entry.summary, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                elif hasattr(entry, "description"):
                    soup = BeautifulSoup(entry.description, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                elif hasattr(entry, "content") and entry.content:
                    soup = BeautifulSoup(entry.content[0].value, "html.parser")
                    description = soup.get_text()[:100] + "..." if len(soup.get_text()) > 100 else soup.get_text()
                
                print(f"æ·»åŠ ç§‘æŠ€æ–‡ç« : {title} - URL: {url}")
                
                tech_news.append({
                    "title": title,
                    "url": url,
                    "description": description
                })
                
                if len(tech_news) >= 3:
                    break
            
            if len(tech_news) >= 3:
                break
                
            time.sleep(2)
            
        except Exception as e:
            print(f"è·å–RSSæº {feed_url} å¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            if isinstance(e, requests.exceptions.RequestException):
                print(f"è¯·æ±‚è¯¦æƒ…: {e.request.method} {e.request.url}")
                if hasattr(e, 'response') and e.response:
                    print(f"å“åº”çŠ¶æ€ç : {e.response.status_code}")
                    print(f"å“åº”å¤´: {e.response.headers}")
            continue
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œä½¿ç”¨å¤‡ç”¨æ–°é—»
    if len(tech_news) == 0:
        print("æœªèƒ½è·å–ä»»ä½•ç§‘æŠ€æ–°é—»ï¼Œä½¿ç”¨å¤‡ç”¨å†…å®¹")
        tech_news = [
            {
                "title": "è‹¹æœå‘å¸ƒæ–°ä¸€ä»£M3èŠ¯ç‰‡ï¼Œæ€§èƒ½å¤§å¹…æå‡",
                "url": "https://www.apple.com/newsroom/",
                "description": "æ–°èŠ¯ç‰‡é‡‡ç”¨å…ˆè¿›å·¥è‰ºï¼Œèƒ½æ•ˆæ¯”åˆ›å†å²æ–°é«˜"
            },
            # ... å…¶ä»–å¤‡ç”¨æ–°é—» ...
        ]
    
    print(f"æœ€ç»ˆè·å–åˆ° {len(tech_news)} æ¡ç§‘æŠ€æ–°é—»")
    for news in tech_news:
        print(f"- {news['title']} ({news['url']})")
    
    return tech_news[:3]  # è¿”å›æœ€å¤š3æ¡æ–°é—»

def get_arxiv_papers(category, max_results=3):
    """è·å– arXiv ç‰¹å®šç±»åˆ«çš„æœ€æ–°è®ºæ–‡"""
    base_url = "http://export.arxiv.org/api/query?"
    
    # æ„å»ºæŸ¥è¯¢å‚æ•°
    query_params = {
        'search_query': f'cat:{category}',
        'sortBy': 'submittedDate',
        'sortOrder': 'descending',
        'max_results': max_results
    }
    
    # æ„å»ºå®Œæ•´ URL
    query_url = base_url + urllib.parse.urlencode(query_params)
    
    try:
        response = requests.get(query_url)
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            title = entry.title
            url = entry.link
            
            # è·å–æ‘˜è¦å¹¶æ¸…ç†æ ¼å¼
            if hasattr(entry, "summary"):
                soup = BeautifulSoup(entry.summary, "html.parser")
                summary = soup.get_text()
                # æˆªæ–­æ‘˜è¦
                description = summary[:150] + "..." if len(summary) > 150 else summary
            else:
                description = "æ— æ‘˜è¦"
            
            # è·å–ä½œè€…
            authors = ", ".join([author.name for author in entry.authors]) if hasattr(entry, "authors") else "æœªçŸ¥ä½œè€…"
            
            papers.append({
                "title": title,
                "url": url,
                "description": description,
                "authors": authors
            })
        
        return papers
    except Exception as e:
        print(f"è·å– arXiv è®ºæ–‡å¤±è´¥: {e}")
        return []

def update_readme():
    """æ›´æ–°README.mdæ–‡ä»¶"""
    try:
        with open('README.md', 'r', encoding='utf-8') as file:
            content = file.read()
    except FileNotFoundError:
        content = "# æŠ€æœ¯è§†é‡åŠ©æ‰‹\n\nè‡ªåŠ¨æ›´æ–°çš„æŠ€æœ¯èµ„è®¯å’Œèµ„æº\n\n"
    
    # æ›´æ–°æ—¥æœŸ
    today = datetime.now().strftime("%Y-%m-%d")
    date_pattern = r"## ä»Šæ—¥æ›´æ–° \(\d{4}-\d{2}-\d{2}\)"
    if re.search(date_pattern, content):
        content = re.sub(date_pattern, f"## ä»Šæ—¥æ›´æ–° ({today})", content)
    else:
        content += f"\n## ä»Šæ—¥æ›´æ–° ({today})\n\n"
    
    # æ›´æ–°åè¨€
    quote = get_programming_quote()
    quote_pattern = r"### ä»Šæ—¥åè¨€\n\n> .*?\n"
    if re.search(quote_pattern, content, re.DOTALL):
        content = re.sub(quote_pattern, f"### ä»Šæ—¥åè¨€\n\n> {quote}\n", content)
    else:
        content += f"### ä»Šæ—¥åè¨€\n\n> {quote}\n\n"
    
    # æ›´æ–°AIæ–°é—»ï¼ˆå¸¦æƒ…æ„Ÿåˆ†æï¼‰
    try:
        ai_news = get_ai_news_from_rss()
        if ai_news and sentiment_analyzer:
            # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
            analyzed_ai_news = sentiment_analyzer.analyze_news_batch(ai_news)
            ai_news_section = "### AI æŠ€æœ¯åŠ¨æ€\n\n"

            for news in analyzed_ai_news:
                sentiment_emoji = sentiment_analyzer.get_sentiment_emoji(news['sentiment']['sentiment'])
                hotness_level = news['hotness']['level']
                ai_news_section += f"- {sentiment_emoji} [{news['title']}]({news['url']}) {hotness_level}\n"
                ai_news_section += f"  {news['description']}\n"

            # æ‰“å°å°†è¦æ›´æ–°çš„å†…å®¹
            print("å°†æ›´æ–° AI æŠ€æœ¯åŠ¨æ€ä¸º:")
            print(ai_news_section)
        elif ai_news:
            # æ²¡æœ‰æƒ…æ„Ÿåˆ†æå™¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
            ai_news_section = "### AI æŠ€æœ¯åŠ¨æ€\n\n"
            for news in ai_news:
                ai_news_section += f"- [{news['title']}]({news['url']}) - {news['description']}\n"
        else:
            ai_news_section = "### AI æŠ€æœ¯åŠ¨æ€\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    except Exception as e:
        print(f"æ›´æ–° AI æ–°é—»æ—¶å‡ºé”™: {e}")
        ai_news_section = "### AI æŠ€æœ¯åŠ¨æ€\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    
    # å®šä¹‰ç¼ºå¤±çš„æ­£åˆ™è¡¨è¾¾å¼
    ai_pattern = r"### AI æŠ€æœ¯åŠ¨æ€\n\n[\s\S]*?(?=\n\n###|\Z)"
    
    if re.search(ai_pattern, content):
        content = re.sub(ai_pattern, ai_news_section, content)
        print("å·²æ›¿æ¢ AI æŠ€æœ¯åŠ¨æ€å†…å®¹")
    else:
        content += ai_news_section + "\n\n"
        print("å·²æ·»åŠ  AI æŠ€æœ¯åŠ¨æ€å†…å®¹")
    
    # æ›´æ–°ç½‘ç»œå®‰å…¨æ–°é—»
    try:
        security_news = get_cybersecurity_news_from_rss()
        if security_news:
            security_news_section = "### ç½‘ç»œå®‰å…¨èµ„è®¯\n\n"
            for news in security_news:
                security_news_section += f"- [{news['title']}]({news['url']}) - {news['description']}\n"
            
            # æ‰“å°å°†è¦æ›´æ–°çš„å†…å®¹
            print("å°†æ›´æ–°ç½‘ç»œå®‰å…¨èµ„è®¯ä¸º:")
            print(security_news_section)
        else:
            security_news_section = "### ç½‘ç»œå®‰å…¨èµ„è®¯\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    except Exception as e:
        print(f"æ›´æ–°ç½‘ç»œå®‰å…¨æ–°é—»æ—¶å‡ºé”™: {e}")
        security_news_section = "### ç½‘ç»œå®‰å…¨èµ„è®¯\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
    security_pattern = r"### ç½‘ç»œå®‰å…¨(æç¤º|èµ„è®¯)\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(security_pattern, content, re.DOTALL):
        content = re.sub(security_pattern, security_news_section, content)
        print("å·²æ›¿æ¢ç½‘ç»œå®‰å…¨èµ„è®¯å†…å®¹")
    else:
        content += security_news_section + "\n\n"
        print("å·²æ·»åŠ ç½‘ç»œå®‰å…¨èµ„è®¯å†…å®¹")
    
    # æ›´æ–°å°±ä¸šè¶‹åŠ¿
    job_trend = get_tech_job_trends()
    job_pattern = r"### æŠ€æœ¯å°±ä¸šè¶‹åŠ¿\n\n.*?\n\n"
    if re.search(job_pattern, content, re.DOTALL):
        content = re.sub(job_pattern, f"### æŠ€æœ¯å°±ä¸šè¶‹åŠ¿\n\n{job_trend}\n\n", content)
    else:
        content += f"### æŠ€æœ¯å°±ä¸šè¶‹åŠ¿\n\n{job_trend}\n\n"
    
    # æ›´æ–°ç§‘æŠ€æ–°é—»ï¼ˆå¸¦æƒ…æ„Ÿåˆ†æï¼‰
    try:
        tech_news = get_tech_news_from_rss()
        if tech_news and sentiment_analyzer:
            # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
            analyzed_tech_news = sentiment_analyzer.analyze_news_batch(tech_news)
            tech_news_section = "### ç§‘æŠ€çƒ­ç‚¹\n\n"

            for news in analyzed_tech_news:
                sentiment_emoji = sentiment_analyzer.get_sentiment_emoji(news['sentiment']['sentiment'])
                hotness_level = news['hotness']['level']
                tech_news_section += f"- {sentiment_emoji} [{news['title']}]({news['url']}) {hotness_level}\n"
                tech_news_section += f"  {news['description']}\n"

            # æ‰“å°å°†è¦æ›´æ–°çš„å†…å®¹
            print("å°†æ›´æ–°ç§‘æŠ€çƒ­ç‚¹ä¸º:")
            print(tech_news_section)
        elif tech_news:
            # æ²¡æœ‰æƒ…æ„Ÿåˆ†æå™¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
            tech_news_section = "### ç§‘æŠ€çƒ­ç‚¹\n\n"
            for news in tech_news:
                tech_news_section += f"- [{news['title']}]({news['url']}) - {news['description']}\n"
        else:
            tech_news_section = "### ç§‘æŠ€çƒ­ç‚¹\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    except Exception as e:
        print(f"æ›´æ–°ç§‘æŠ€æ–°é—»æ—¶å‡ºé”™: {e}")
        tech_news_section = "### ç§‘æŠ€çƒ­ç‚¹\n\n- RSS è®¢é˜…æºæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
    tech_pattern = r"### ç§‘æŠ€çƒ­ç‚¹\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(tech_pattern, content):
        content = re.sub(tech_pattern, tech_news_section, content)
        print("å·²æ›¿æ¢ç§‘æŠ€çƒ­ç‚¹å†…å®¹")
    else:
        content += tech_news_section + "\n\n"
        print("å·²æ·»åŠ ç§‘æŠ€çƒ­ç‚¹å†…å®¹")
    
    # æ›´æ–°GitHubè¶‹åŠ¿é¡¹ç›®
    try:
        trending_repos = get_github_trending()
        if trending_repos:
            trending_section = "### GitHub è¶‹åŠ¿é¡¹ç›®\n\n"
            for repo in trending_repos:
                trending_section += f"- [{repo['name']}]({repo['url']}) - {repo['description']}\n"
        else:
            trending_section = "### GitHub è¶‹åŠ¿é¡¹ç›®\n\n- GitHub è¶‹åŠ¿æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    except Exception as e:
        print(f"è·å–GitHubè¶‹åŠ¿é¡¹ç›®å¤±è´¥: {e}")
        trending_section = "### GitHub è¶‹åŠ¿é¡¹ç›®\n\n- GitHub è¶‹åŠ¿æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n"
    
    # å®šä¹‰ç¼ºå¤±çš„æ­£åˆ™è¡¨è¾¾å¼
    trending_pattern = r"### GitHub è¶‹åŠ¿é¡¹ç›®\n\n[\s\S]*?(?=\n\n###|\Z)"
    
    if re.search(trending_pattern, content, re.DOTALL):
        content = re.sub(trending_pattern, trending_section + "\n\n", content)
    else:
        content += trending_section + "\n\n"
    
    # æ›´æ–° AI ç ”ç©¶è®ºæ–‡
    try:
        ai_papers = get_arxiv_papers("cs.AI", 3)  # äººå·¥æ™ºèƒ½ç±»åˆ«
        if ai_papers:
            ai_papers_section = "### AI ç ”ç©¶è®ºæ–‡\n\n"
            for paper in ai_papers:
                ai_papers_section += f"- [{paper['title']}]({paper['url']}) - {paper['authors']}\n  {paper['description']}\n\n"
            
            # æ‰“å°å°†è¦æ›´æ–°çš„å†…å®¹
            print("å°†æ›´æ–° AI ç ”ç©¶è®ºæ–‡ä¸º:")
            print(ai_papers_section)
        else:
            ai_papers_section = "### AI ç ”ç©¶è®ºæ–‡\n\n- arXiv è®ºæ–‡æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n\n"
    except Exception as e:
        print(f"æ›´æ–° AI è®ºæ–‡æ—¶å‡ºé”™: {e}")
        ai_papers_section = "### AI ç ”ç©¶è®ºæ–‡\n\n- arXiv è®ºæ–‡æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n\n"
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
    ai_papers_pattern = r"### AI ç ”ç©¶è®ºæ–‡\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(ai_papers_pattern, content, re.DOTALL):
        content = re.sub(ai_papers_pattern, ai_papers_section, content)
        print("å·²æ›¿æ¢ AI ç ”ç©¶è®ºæ–‡å†…å®¹")
    else:
        content += ai_papers_section
    
    # æ›´æ–°ç½‘ç»œå®‰å…¨ç ”ç©¶è®ºæ–‡
    try:
        # ç”±äº arXiv æ²¡æœ‰ä¸“é—¨çš„ç½‘ç»œå®‰å…¨ç±»åˆ«ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯†ç å­¦å’Œç³»ç»Ÿå®‰å…¨ç›¸å…³ç±»åˆ«
        security_papers = get_arxiv_papers("cs.CR", 3)  # å¯†ç å­¦ä¸å®‰å…¨ç±»åˆ«
        if security_papers:
            security_papers_section = "### ç½‘ç»œå®‰å…¨ç ”ç©¶è®ºæ–‡\n\n"
            for paper in security_papers:
                security_papers_section += f"- [{paper['title']}]({paper['url']}) - {paper['authors']}\n  {paper['description']}\n\n"
        else:
            security_papers_section = "### ç½‘ç»œå®‰å…¨ç ”ç©¶è®ºæ–‡\n\n- arXiv è®ºæ–‡æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n\n"
    except Exception as e:
        print(f"æ›´æ–°ç½‘ç»œå®‰å…¨è®ºæ–‡æ—¶å‡ºé”™: {e}")
        security_papers_section = "### ç½‘ç»œå®‰å…¨ç ”ç©¶è®ºæ–‡\n\n- arXiv è®ºæ–‡æ•°æ®æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†æŸ¥çœ‹\n\n"
    
    security_papers_pattern = r"### ç½‘ç»œå®‰å…¨ç ”ç©¶è®ºæ–‡\n\n- \[.*?\n\n"
    if re.search(security_papers_pattern, content, re.DOTALL):
        content = re.sub(security_papers_pattern, security_papers_section, content)
    else:
        content += security_papers_section
    
    # æ·»åŠ æ–°åŠŸèƒ½å†…å®¹

    # æŠ€æœ¯è‚¡ç¥¨è¿½è¸ª
    try:
        tech_stocks = get_tech_stocks()
        if tech_stocks:
            stocks_section = "### ğŸ“ˆ ç§‘æŠ€è‚¡ç¥¨è¿½è¸ª\n\n"
            for stock in tech_stocks:
                change_symbol = "ğŸ“ˆ" if stock['change'] >= 0 else "ğŸ“‰"
                stocks_section += f"- **{stock['symbol']}**: ${stock['price']:.2f} {change_symbol} {stock['change']:+.2f} ({stock['change_percent']:+.1f}%)\n"
        else:
            stocks_section = "### ğŸ“ˆ ç§‘æŠ€è‚¡ç¥¨è¿½è¸ª\n\n- è‚¡ç¥¨æ•°æ®æš‚æ—¶ä¸å¯ç”¨\n"
    except Exception as e:
        print(f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
        stocks_section = "### ğŸ“ˆ ç§‘æŠ€è‚¡ç¥¨è¿½è¸ª\n\n- è‚¡ç¥¨æ•°æ®æš‚æ—¶ä¸å¯ç”¨\n"

    stocks_pattern = r"### ğŸ“ˆ ç§‘æŠ€è‚¡ç¥¨è¿½è¸ª\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(stocks_pattern, content):
        content = re.sub(stocks_pattern, stocks_section, content)
    else:
        content += stocks_section + "\n\n"

    # å¼€å‘è€…å·¥å…·æ¨è
    try:
        dev_tools = get_dev_tools()
        tools_section = "### ğŸ› ï¸ å¼€å‘è€…å·¥å…·æ¨è\n\n"
        for tool in dev_tools:
            tools_section += f"- **[{tool['name']}]({tool['url']})** ({tool['category']}) - {tool['description']}\n"
    except Exception as e:
        print(f"è·å–å¼€å‘å·¥å…·å¤±è´¥: {e}")
        tools_section = "### ğŸ› ï¸ å¼€å‘è€…å·¥å…·æ¨è\n\n- å·¥å…·æ¨èæš‚æ—¶ä¸å¯ç”¨\n"

    tools_pattern = r"### ğŸ› ï¸ å¼€å‘è€…å·¥å…·æ¨è\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(tools_pattern, content):
        content = re.sub(tools_pattern, tools_section, content)
    else:
        content += tools_section + "\n\n"

    # ç¼–ç¨‹æŒ‘æˆ˜
    try:
        challenge = get_coding_challenge()
        challenge_section = f"### ğŸ¯ ä»Šæ—¥ç¼–ç¨‹æŒ‘æˆ˜\n\n"
        challenge_section += f"**{challenge['title']}** (éš¾åº¦: {challenge['difficulty']})\n\n"
        challenge_section += f"{challenge['description']}\n\n"
        challenge_section += f"æ ‡ç­¾: {', '.join(challenge['tags'])}\n"
    except Exception as e:
        print(f"è·å–ç¼–ç¨‹æŒ‘æˆ˜å¤±è´¥: {e}")
        challenge_section = "### ğŸ¯ ä»Šæ—¥ç¼–ç¨‹æŒ‘æˆ˜\n\n- ç¼–ç¨‹æŒ‘æˆ˜æš‚æ—¶ä¸å¯ç”¨\n"

    challenge_pattern = r"### ğŸ¯ ä»Šæ—¥ç¼–ç¨‹æŒ‘æˆ˜\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(challenge_pattern, content):
        content = re.sub(challenge_pattern, challenge_section, content)
    else:
        content += challenge_section + "\n\n"

    # ç§»åŠ¨å¼€å‘åŠ¨æ€
    try:
        mobile_news = get_mobile_dev_news()
        mobile_section = "### ğŸ“± ç§»åŠ¨å¼€å‘åŠ¨æ€\n\n"
        for news in mobile_news:
            mobile_section += f"- [{news['title']}]({news['url']}) - {news['description']}\n"
    except Exception as e:
        print(f"è·å–ç§»åŠ¨å¼€å‘æ–°é—»å¤±è´¥: {e}")
        mobile_section = "### ğŸ“± ç§»åŠ¨å¼€å‘åŠ¨æ€\n\n- ç§»åŠ¨å¼€å‘èµ„è®¯æš‚æ—¶ä¸å¯ç”¨\n"

    mobile_pattern = r"### ğŸ“± ç§»åŠ¨å¼€å‘åŠ¨æ€\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(mobile_pattern, content):
        content = re.sub(mobile_pattern, mobile_section, content)
    else:
        content += mobile_section + "\n\n"

    # æŠ€æœ¯è¶£é—»
    try:
        trivia = get_tech_trivia()
        trivia_section = f"### ğŸª æŠ€æœ¯è¶£é—»\n\n{trivia}\n"
    except Exception as e:
        print(f"è·å–æŠ€æœ¯è¶£é—»å¤±è´¥: {e}")
        trivia_section = "### ğŸª æŠ€æœ¯è¶£é—»\n\n- æŠ€æœ¯è¶£é—»æš‚æ—¶ä¸å¯ç”¨\n"

    trivia_pattern = r"### ğŸª æŠ€æœ¯è¶£é—»\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(trivia_pattern, content):
        content = re.sub(trivia_pattern, trivia_section, content)
    else:
        content += trivia_section + "\n\n"

    # æŠ€æœ¯ä¹¦ç±æ¨è
    try:
        book = get_tech_books()
        book_section = f"### ğŸ“š æŠ€æœ¯ä¹¦ç±æ¨è\n\n"
        book_section += f"**ã€Š{book['title']}ã€‹** - {book['author']}\n\n"
        book_section += f"{book['description']} (åˆ†ç±»: {book['category']})\n"
    except Exception as e:
        print(f"è·å–ä¹¦ç±æ¨èå¤±è´¥: {e}")
        book_section = "### ğŸ“š æŠ€æœ¯ä¹¦ç±æ¨è\n\n- ä¹¦ç±æ¨èæš‚æ—¶ä¸å¯ç”¨\n"

    book_pattern = r"### ğŸ“š æŠ€æœ¯ä¹¦ç±æ¨è\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(book_pattern, content):
        content = re.sub(book_pattern, book_section, content)
    else:
        content += book_section + "\n\n"

    # æŠ€æœ¯ä¼šè®®æ—¥å†
    try:
        conferences = get_tech_conferences()
        if conferences:
            conf_section = "### ğŸŒ å³å°†ä¸¾è¡Œçš„æŠ€æœ¯ä¼šè®®\n\n"
            for conf in conferences:
                conf_section += f"- **{conf['name']}** ({conf['date']}) - {conf['location']}\n"
                conf_section += f"  ä¸»é¢˜: {', '.join(conf['topics'])} | è¿˜æœ‰ {conf['days_until']} å¤©\n"
        else:
            conf_section = "### ğŸŒ å³å°†ä¸¾è¡Œçš„æŠ€æœ¯ä¼šè®®\n\n- æš‚æ— å³å°†ä¸¾è¡Œçš„ä¼šè®®ä¿¡æ¯\n"
    except Exception as e:
        print(f"è·å–æŠ€æœ¯ä¼šè®®å¤±è´¥: {e}")
        conf_section = "### ğŸŒ å³å°†ä¸¾è¡Œçš„æŠ€æœ¯ä¼šè®®\n\n- ä¼šè®®ä¿¡æ¯æš‚æ—¶ä¸å¯ç”¨\n"

    conf_pattern = r"### ğŸŒ å³å°†ä¸¾è¡Œçš„æŠ€æœ¯ä¼šè®®\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(conf_pattern, content):
        content = re.sub(conf_pattern, conf_section, content)
    else:
        content += conf_section + "\n\n"

    # åˆ›ä¸šå…¬å¸åŠ¨æ€
    try:
        startup_news = get_startup_news()
        startup_section = "### ğŸš€ åˆ›ä¸šå…¬å¸åŠ¨æ€\n\n"
        for news in startup_news:
            startup_section += f"- **{news['company']}**: {news['news']}"
            if news['amount'] != 'N/A':
                startup_section += f" ({news['amount']})"
            startup_section += f" - {news['description']}\n"
    except Exception as e:
        print(f"è·å–åˆ›ä¸šåŠ¨æ€å¤±è´¥: {e}")
        startup_section = "### ğŸš€ åˆ›ä¸šå…¬å¸åŠ¨æ€\n\n- åˆ›ä¸šåŠ¨æ€æš‚æ—¶ä¸å¯ç”¨\n"

    startup_pattern = r"### ğŸš€ åˆ›ä¸šå…¬å¸åŠ¨æ€\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(startup_pattern, content):
        content = re.sub(startup_pattern, startup_section, content)
    else:
        content += startup_section + "\n\n"

    # è®¾è®¡èµ„æºæ¨è
    try:
        design_resources = get_design_resources()
        design_section = "### ğŸ¨ è®¾è®¡èµ„æºæ¨è\n\n"
        for resource in design_resources:
            design_section += f"- **[{resource['name']}]({resource['url']})** ({resource['type']}) - {resource['description']}\n"
    except Exception as e:
        print(f"è·å–è®¾è®¡èµ„æºå¤±è´¥: {e}")
        design_section = "### ğŸ¨ è®¾è®¡èµ„æºæ¨è\n\n- è®¾è®¡èµ„æºæš‚æ—¶ä¸å¯ç”¨\n"

    design_pattern = r"### ğŸ¨ è®¾è®¡èµ„æºæ¨è\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(design_pattern, content):
        content = re.sub(design_pattern, design_section, content)
    else:
        content += design_section + "\n\n"

    # å­¦ä¹ è·¯å¾„æ¨è
    try:
        learning_path = get_learning_path()
        learning_section = f"### ğŸ“ æŠ€èƒ½å­¦ä¹ è·¯å¾„\n\n"
        learning_section += f"**{learning_path['skill']}** (éš¾åº¦: {learning_path['level']}, é¢„è®¡æ—¶é—´: {learning_path['duration']})\n\n"
        learning_section += f"å­¦ä¹ æ­¥éª¤: {' â†’ '.join(learning_path['steps'])}\n"
    except Exception as e:
        print(f"è·å–å­¦ä¹ è·¯å¾„å¤±è´¥: {e}")
        learning_section = "### ğŸ“ æŠ€èƒ½å­¦ä¹ è·¯å¾„\n\n- å­¦ä¹ è·¯å¾„æ¨èæš‚æ—¶ä¸å¯ç”¨\n"

    learning_pattern = r"### ğŸ“ æŠ€èƒ½å­¦ä¹ è·¯å¾„\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(learning_pattern, content):
        content = re.sub(learning_pattern, learning_section, content)
    else:
        content += learning_section + "\n\n"

    # ç¼–ç¨‹éŸ³ä¹æ¨è
    try:
        music = get_programming_music()
        music_section = f"### ğŸµ ç¼–ç¨‹éŸ³ä¹æ¨è\n\n"
        music_section += f"**{music['title']}** - {music['artist']}\n\n"
        music_section += f"ç±»å‹: {music['genre']} | {music['description']}\n"
    except Exception as e:
        print(f"è·å–éŸ³ä¹æ¨èå¤±è´¥: {e}")
        music_section = "### ğŸµ ç¼–ç¨‹éŸ³ä¹æ¨è\n\n- éŸ³ä¹æ¨èæš‚æ—¶ä¸å¯ç”¨\n"

    music_pattern = r"### ğŸµ ç¼–ç¨‹éŸ³ä¹æ¨è\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(music_pattern, content):
        content = re.sub(music_pattern, music_section, content)
    else:
        content += music_section + "\n\n"

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    stats_section = f"### ğŸ“Š ä»Šæ—¥ç»Ÿè®¡\n\n"
    stats_section += f"- ğŸ“… æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    stats_section += f"- ğŸ”„ è‡ªåŠ¨æ›´æ–°: æ¯æ—¥ 08:00 (UTC+8)\n"
    stats_section += f"- ğŸ“ˆ åŠŸèƒ½æ¨¡å—: 15+ ä¸ªæ´»è·ƒåŠŸèƒ½\n"
    stats_section += f"- ğŸŒŸ æ•°æ®æº: å¤šä¸ªRSSæºå’ŒAPIæ¥å£\n"

    stats_pattern = r"### ğŸ“Š ä»Šæ—¥ç»Ÿè®¡\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(stats_pattern, content):
        content = re.sub(stats_pattern, stats_section, content)
    else:
        content += stats_section + "\n\n"

    # æŠ€æœ¯çƒ­è¯è¶‹åŠ¿
    try:
        tech_trends = get_tech_trends()
        trends_section = "### ğŸ”¥ æŠ€æœ¯çƒ­è¯è¶‹åŠ¿\n\n"
        for trend in tech_trends:
            trends_section += f"- **{trend['keyword']}** {trend['trend']} {trend['change']} - {trend['description']}\n"
    except Exception as e:
        print(f"è·å–æŠ€æœ¯è¶‹åŠ¿å¤±è´¥: {e}")
        trends_section = "### ğŸ”¥ æŠ€æœ¯çƒ­è¯è¶‹åŠ¿\n\n- è¶‹åŠ¿æ•°æ®æš‚æ—¶ä¸å¯ç”¨\n"

    trends_pattern = r"### ğŸ”¥ æŠ€æœ¯çƒ­è¯è¶‹åŠ¿\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(trends_pattern, content):
        content = re.sub(trends_pattern, trends_section, content)
    else:
        content += trends_section + "\n\n"

    # å¼€æºé¡¹ç›®èšç„¦
    try:
        spotlight_project = get_open_source_spotlight()
        spotlight_section = f"### â­ å¼€æºé¡¹ç›®èšç„¦\n\n"
        spotlight_section += f"**{spotlight_project['name']}** ({spotlight_project['language']}) - â­ {spotlight_project['stars']}\n\n"
        spotlight_section += f"{spotlight_project['description']}\n\n"
        spotlight_section += f"ğŸ’¡ äº®ç‚¹: {spotlight_project['why_interesting']}\n"
    except Exception as e:
        print(f"è·å–å¼€æºé¡¹ç›®å¤±è´¥: {e}")
        spotlight_section = "### â­ å¼€æºé¡¹ç›®èšç„¦\n\n- é¡¹ç›®ä¿¡æ¯æš‚æ—¶ä¸å¯ç”¨\n"

    spotlight_pattern = r"### â­ å¼€æºé¡¹ç›®èšç„¦\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(spotlight_pattern, content):
        content = re.sub(spotlight_pattern, spotlight_section, content)
    else:
        content += spotlight_section + "\n\n"

    # è–ªèµ„æŠ¥å‘Š
    try:
        salary_info = get_tech_salary_info()
        salary_section = f"### ğŸ’° æŠ€æœ¯è–ªèµ„å¿«æŠ¥\n\n"
        salary_section += f"**{salary_info['position']}** ({salary_info['level']}) {salary_info['trend']}\n\n"
        salary_section += f"è–ªèµ„èŒƒå›´: {salary_info['salary_range']} | çƒ­é—¨æŠ€èƒ½: {', '.join(salary_info['hot_skills'])}\n"
    except Exception as e:
        print(f"è·å–è–ªèµ„ä¿¡æ¯å¤±è´¥: {e}")
        salary_section = "### ğŸ’° æŠ€æœ¯è–ªèµ„å¿«æŠ¥\n\n- è–ªèµ„ä¿¡æ¯æš‚æ—¶ä¸å¯ç”¨\n"

    salary_pattern = r"### ğŸ’° æŠ€æœ¯è–ªèµ„å¿«æŠ¥\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(salary_pattern, content):
        content = re.sub(salary_pattern, salary_section, content)
    else:
        content += salary_section + "\n\n"

    # ç¨‹åºå‘˜ç¬‘è¯
    try:
        joke = get_developer_joke()
        joke_section = f"### ğŸ˜„ ç¨‹åºå‘˜ç¬‘è¯\n\n{joke}\n"
    except Exception as e:
        print(f"è·å–ç¬‘è¯å¤±è´¥: {e}")
        joke_section = "### ğŸ˜„ ç¨‹åºå‘˜ç¬‘è¯\n\n- ç¬‘è¯æš‚æ—¶ä¸å¯ç”¨\n"

    joke_pattern = r"### ğŸ˜„ ç¨‹åºå‘˜ç¬‘è¯\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(joke_pattern, content):
        content = re.sub(joke_pattern, joke_section, content)
    else:
        content += joke_section + "\n\n"

    # GitHubç»Ÿè®¡ä¿¡æ¯
    try:
        github_stats = get_github_stats()
        if github_stats:
            github_section = "### ğŸ™ GitHub ç”Ÿæ€ç»Ÿè®¡\n\n"
            github_section += f"- ğŸ“¦ æ€»ä»“åº“æ•°: {github_stats['total_repos']}\n"
            github_section += f"- ğŸ‘¥ æ´»è·ƒå¼€å‘è€…: {github_stats['active_developers']}\n"
            github_section += f"- ğŸ’» æ¯æ—¥æäº¤: {github_stats['daily_commits']}\n"
            github_section += f"- ğŸŒ ç¼–ç¨‹è¯­è¨€: {github_stats['languages_used']}\n"
            github_section += f"- ğŸ”“ å¼€æºé¡¹ç›®: {github_stats['open_source_projects']}\n"
        else:
            github_section = "### ğŸ™ GitHub ç”Ÿæ€ç»Ÿè®¡\n\n- ç»Ÿè®¡æ•°æ®æš‚æ—¶ä¸å¯ç”¨\n"
    except Exception as e:
        print(f"è·å–GitHubç»Ÿè®¡å¤±è´¥: {e}")
        github_section = "### ğŸ™ GitHub ç”Ÿæ€ç»Ÿè®¡\n\n- ç»Ÿè®¡æ•°æ®æš‚æ—¶ä¸å¯ç”¨\n"

    github_pattern = r"### ğŸ™ GitHub ç”Ÿæ€ç»Ÿè®¡\n\n[\s\S]*?(?=\n\n###|\Z)"
    if re.search(github_pattern, content):
        content = re.sub(github_pattern, github_section, content)
    else:
        content += github_section + "\n\n"

    # äº’åŠ¨åŠŸèƒ½éƒ¨åˆ†
    if interactive_features:
        # æ¯å‘¨æŒ‘æˆ˜
        try:
            weekly_challenge = interactive_features.get_current_week_challenge()
            challenge_section = f"### ğŸ† æœ¬å‘¨æŠ€æœ¯æŒ‘æˆ˜\n\n"
            challenge_section += f"**{weekly_challenge['title']}** (éš¾åº¦: {weekly_challenge['difficulty']})\n\n"
            challenge_section += f"{weekly_challenge['description']}\n\n"
            challenge_section += f"ğŸ·ï¸ æ ‡ç­¾: {', '.join(weekly_challenge['tags'])} | "
            challenge_section += f"â±ï¸ é¢„è®¡æ—¶é—´: {weekly_challenge['estimated_time']} | "
            challenge_section += f"ğŸ¯ å¥–åŠ±ç§¯åˆ†: {weekly_challenge['reward_points']}\n"
        except Exception as e:
            print(f"è·å–æ¯å‘¨æŒ‘æˆ˜å¤±è´¥: {e}")
            challenge_section = "### ğŸ† æœ¬å‘¨æŠ€æœ¯æŒ‘æˆ˜\n\n- æŒ‘æˆ˜ä¿¡æ¯æš‚æ—¶ä¸å¯ç”¨\n"

        weekly_challenge_pattern = r"### ğŸ† æœ¬å‘¨æŠ€æœ¯æŒ‘æˆ˜\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(weekly_challenge_pattern, content):
            content = re.sub(weekly_challenge_pattern, challenge_section, content)
        else:
            content += challenge_section + "\n\n"

        # æŠ€æœ¯å°æµ‹éªŒ
        try:
            quiz = interactive_features.generate_tech_quiz()
            quiz_section = f"### ğŸ§  æŠ€æœ¯å°æµ‹éªŒ\n\n"
            quiz_section += f"**é—®é¢˜**: {quiz['question']}\n\n"
            for i, option in enumerate(quiz['options']):
                quiz_section += f"{chr(65+i)}. {option}\n"
            quiz_section += f"\nğŸ’¡ ç­”æ¡ˆå°†åœ¨æ˜å¤©å…¬å¸ƒ\n"
        except Exception as e:
            print(f"è·å–æŠ€æœ¯æµ‹éªŒå¤±è´¥: {e}")
            quiz_section = "### ğŸ§  æŠ€æœ¯å°æµ‹éªŒ\n\n- æµ‹éªŒå†…å®¹æš‚æ—¶ä¸å¯ç”¨\n"

        quiz_pattern = r"### ğŸ§  æŠ€æœ¯å°æµ‹éªŒ\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(quiz_pattern, content):
            content = re.sub(quiz_pattern, quiz_section, content)
        else:
            content += quiz_section + "\n\n"

        # ç¼–ç¨‹å°è´´å£«
        try:
            tip = interactive_features.get_coding_tip_of_day()
            tip_section = f"### ğŸ’¡ ä»Šæ—¥ç¼–ç¨‹å°è´´å£«\n\n"
            tip_section += f"**{tip['title']}**\n\n"
            tip_section += f"{tip['content']}\n\n"
            if 'example' in tip:
                tip_section += f"```\n{tip['example']}\n```\n"
        except Exception as e:
            print(f"è·å–ç¼–ç¨‹å°è´´å£«å¤±è´¥: {e}")
            tip_section = "### ğŸ’¡ ä»Šæ—¥ç¼–ç¨‹å°è´´å£«\n\n- å°è´´å£«æš‚æ—¶ä¸å¯ç”¨\n"

        tip_pattern = r"### ğŸ’¡ ä»Šæ—¥ç¼–ç¨‹å°è´´å£«\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(tip_pattern, content):
            content = re.sub(tip_pattern, tip_section, content)
        else:
            content += tip_section + "\n\n"

        # èŒä¸šå»ºè®®
        try:
            career_advice = interactive_features.get_tech_career_advice()
            career_section = f"### ğŸš€ èŒä¸šå‘å±•å»ºè®®\n\n"
            career_section += f"**{career_advice['category']}**: {career_advice['advice']}\n\n"
            career_section += f"ğŸ“‹ è¡ŒåŠ¨å»ºè®®: {career_advice['action']}\n"
        except Exception as e:
            print(f"è·å–èŒä¸šå»ºè®®å¤±è´¥: {e}")
            career_section = "### ğŸš€ èŒä¸šå‘å±•å»ºè®®\n\n- èŒä¸šå»ºè®®æš‚æ—¶ä¸å¯ç”¨\n"

        career_pattern = r"### ğŸš€ èŒä¸šå‘å±•å»ºè®®\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(career_pattern, content):
            content = re.sub(career_pattern, career_section, content)
        else:
            content += career_section + "\n\n"

        # æ¯æ—¥æŒ‘æˆ˜å¾½ç« 
        try:
            badge_info = interactive_features.generate_daily_challenge_badge()
            badge_section = f"### ğŸ… ä»Šæ—¥æŒ‘æˆ˜å¾½ç« \n\n"
            badge_section += f"{badge_info['message']}\n\n"
            badge_section += f"æŒ‘æˆ˜ID: `{badge_info['challenge_id']}` | æ—¥æœŸ: {badge_info['date']}\n"
        except Exception as e:
            print(f"è·å–æŒ‘æˆ˜å¾½ç« å¤±è´¥: {e}")
            badge_section = "### ğŸ… ä»Šæ—¥æŒ‘æˆ˜å¾½ç« \n\n- å¾½ç« ä¿¡æ¯æš‚æ—¶ä¸å¯ç”¨\n"

        badge_pattern = r"### ğŸ… ä»Šæ—¥æŒ‘æˆ˜å¾½ç« \n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(badge_pattern, content):
            content = re.sub(badge_pattern, badge_section, content)
        else:
            content += badge_section + "\n\n"

        # æŠ€æœ¯æŠ•ç¥¨
        try:
            poll = interactive_features.get_random_poll()
            poll_section = f"### ğŸ“Š æŠ€æœ¯è¯é¢˜æŠ•ç¥¨\n\n"
            poll_section += f"**{poll['question']}** (åˆ†ç±»: {poll['category']})\n\n"
            for i, option in enumerate(poll['options']):
                poll_section += f"- [ ] {option}\n"
            poll_section += f"\nğŸ’¬ åœ¨Issuesä¸­å‚ä¸è®¨è®ºå’ŒæŠ•ç¥¨ï¼\n"
        except Exception as e:
            print(f"è·å–æŠ€æœ¯æŠ•ç¥¨å¤±è´¥: {e}")
            poll_section = "### ğŸ“Š æŠ€æœ¯è¯é¢˜æŠ•ç¥¨\n\n- æŠ•ç¥¨å†…å®¹æš‚æ—¶ä¸å¯ç”¨\n"

        poll_pattern = r"### ğŸ“Š æŠ€æœ¯è¯é¢˜æŠ•ç¥¨\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(poll_pattern, content):
            content = re.sub(poll_pattern, poll_section, content)
        else:
            content += poll_section + "\n\n"

    # æ–°é—»åˆ†ææ‘˜è¦
    if sentiment_analyzer:
        try:
            # æ”¶é›†æ‰€æœ‰æ–°é—»è¿›è¡Œç»¼åˆåˆ†æ
            all_news = []

            # é‡æ–°è·å–æ–°é—»æ•°æ®è¿›è¡Œåˆ†æ
            try:
                ai_news = get_ai_news_from_rss()
                if ai_news:
                    all_news.extend(ai_news)
            except:
                pass

            try:
                tech_news = get_tech_news_from_rss()
                if tech_news:
                    all_news.extend(tech_news)
            except:
                pass

            if all_news:
                trend_summary = sentiment_analyzer.generate_trend_summary(
                    sentiment_analyzer.analyze_news_batch(all_news)
                )

                analysis_section = f"### ğŸ“ˆ ä»Šæ—¥æ–°é—»åˆ†æ\n\n{trend_summary}\n"
            else:
                analysis_section = "### ğŸ“ˆ ä»Šæ—¥æ–°é—»åˆ†æ\n\n- æš‚æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ†æ\n"

        except Exception as e:
            print(f"ç”Ÿæˆæ–°é—»åˆ†æå¤±è´¥: {e}")
            analysis_section = "### ğŸ“ˆ ä»Šæ—¥æ–°é—»åˆ†æ\n\n- åˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨\n"

        analysis_pattern = r"### ğŸ“ˆ ä»Šæ—¥æ–°é—»åˆ†æ\n\n[\s\S]*?(?=\n\n###|\Z)"
        if re.search(analysis_pattern, content):
            content = re.sub(analysis_pattern, analysis_section, content)
        else:
            content += analysis_section + "\n\n"

    # æ·»åŠ é¡µè„šä¿¡æ¯
    footer_section = "---\n\n"
    footer_section += "### ğŸ¤– å…³äºæ­¤é¡¹ç›®\n\n"
    footer_section += "è¿™æ˜¯ä¸€ä¸ªç”± GitHub Actions é©±åŠ¨çš„è‡ªåŠ¨åŒ–æŠ€æœ¯èµ„è®¯èšåˆé¡¹ç›®ã€‚\n\n"
    footer_section += "- ğŸ”„ **è‡ªåŠ¨æ›´æ–°**: æ¯å¤©è‡ªåŠ¨æŠ“å–æœ€æ–°æŠ€æœ¯èµ„è®¯\n"
    footer_section += "- ğŸŒ **å¤šæºèšåˆ**: æ•´åˆå¤šä¸ªæƒå¨æŠ€æœ¯åª’ä½“å’Œå¹³å°\n"
    footer_section += "- ğŸ¯ **æ™ºèƒ½ç­›é€‰**: AIè¾…åŠ©å†…å®¹ç­›é€‰å’Œåˆ†ç±»\n"
    footer_section += "- ğŸ“Š **æ•°æ®å¯è§†**: è¶‹åŠ¿åˆ†æå’Œç»Ÿè®¡å±•ç¤º\n\n"
    footer_section += "**æ•°æ®æ¥æº**: RSSè®¢é˜…ã€APIæ¥å£ã€ç½‘é¡µæŠ“å–\n\n"
    footer_section += "**æ›´æ–°é¢‘ç‡**: æ¯æ—¥ 08:00 (UTC+8)\n\n"
    footer_section += "**é¡¹ç›®ç»´æŠ¤**: ç”± GitHub Actions è‡ªåŠ¨ç»´æŠ¤ï¼Œæ¬¢è¿ Star â­ å’Œ Fork ğŸ´\n\n"

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¡µè„šï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
    if "### ğŸ¤– å…³äºæ­¤é¡¹ç›®" not in content:
        content += footer_section

    # å†™å…¥æ›´æ–°åçš„å†…å®¹
    try:
        with open('README.md', 'w', encoding='utf-8') as file:
            file.write(content)
        print("README.md å·²æ›´æ–°")
    except Exception as e:
        print(f"å†™å…¥ README.md å¤±è´¥: {e}")

    # ç”ŸæˆHTMLé¡µé¢
    if html_generator:
        try:
            # æ”¶é›†æ•°æ®ç”¨äºHTMLç”Ÿæˆ
            html_data = {
                'ai_news': [],
                'tech_news': [],
                'tech_trends': [],
                'dev_tools': [],
                'github_repos': []
            }

            # é‡æ–°è·å–æ•°æ®
            try:
                ai_news = get_ai_news_from_rss()
                if ai_news:
                    html_data['ai_news'] = ai_news
            except:
                pass

            try:
                tech_news = get_tech_news_from_rss()
                if tech_news:
                    html_data['tech_news'] = tech_news
            except:
                pass

            try:
                html_data['tech_trends'] = get_tech_trends()
            except:
                pass

            try:
                html_data['dev_tools'] = get_dev_tools()
            except:
                pass

            try:
                html_data['github_repos'] = get_github_trending()
            except:
                pass

            # ç”ŸæˆHTML
            html_content = html_generator.generate_html_page(html_data)
            html_generator.save_html_file(html_content, 'docs/index.html')

            print("HTMLé¡µé¢å·²ç”Ÿæˆ")

        except Exception as e:
            print(f"ç”ŸæˆHTMLé¡µé¢å¤±è´¥: {e}")

    # ç”ŸæˆAPIæ•°æ®
    if api_generator:
        try:
            # æ”¶é›†æ‰€æœ‰æ•°æ®ç”¨äºAPI
            api_data = {
                'ai_news': [],
                'tech_news': [],
                'security_news': [],
                'github_repos': [],
                'tech_trends': [],
                'dev_tools': []
            }

            # é‡æ–°è·å–æ•°æ®
            try:
                ai_news = get_ai_news_from_rss()
                if ai_news:
                    api_data['ai_news'] = ai_news
            except:
                pass

            try:
                tech_news = get_tech_news_from_rss()
                if tech_news:
                    api_data['tech_news'] = tech_news
            except:
                pass

            try:
                security_news = get_security_news_from_rss()
                if security_news:
                    api_data['security_news'] = security_news
            except:
                pass

            try:
                api_data['github_repos'] = get_github_trending()
            except:
                pass

            try:
                api_data['tech_trends'] = get_tech_trends()
            except:
                pass

            try:
                api_data['dev_tools'] = get_dev_tools()
            except:
                pass

            # ç”ŸæˆAPIæ–‡ä»¶
            api_generator.save_api_files(api_data)
            print("APIæ•°æ®å·²ç”Ÿæˆ")

        except Exception as e:
            print(f"ç”ŸæˆAPIæ•°æ®å¤±è´¥: {e}")

    # ç”ŸæˆRSSè®¢é˜…æº
    if rss_generator:
        try:
            # ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç”ŸæˆRSS
            rss_data = {
                'ai_news': [],
                'tech_news': [],
                'security_news': [],
                'github_repos': []
            }

            # é‡æ–°è·å–æ•°æ®
            try:
                ai_news = get_ai_news_from_rss()
                if ai_news:
                    rss_data['ai_news'] = ai_news
            except:
                pass

            try:
                tech_news = get_tech_news_from_rss()
                if tech_news:
                    rss_data['tech_news'] = tech_news
            except:
                pass

            try:
                security_news = get_security_news_from_rss()
                if security_news:
                    rss_data['security_news'] = security_news
            except:
                pass

            try:
                github_repos = get_github_trending()
                if github_repos:
                    rss_data['github_repos'] = github_repos
            except:
                pass

            # ç”ŸæˆRSSæ–‡ä»¶
            rss_generator.save_rss_files(rss_data)
            print("RSSè®¢é˜…æºå·²ç”Ÿæˆ")

        except Exception as e:
            print(f"ç”ŸæˆRSSè®¢é˜…æºå¤±è´¥: {e}")

# æ–°å¢åŠŸèƒ½å‡½æ•°

def get_tech_stocks():
    """è·å–ç§‘æŠ€è‚¡ç¥¨ä¿¡æ¯"""
    # ä½¿ç”¨å…è´¹çš„è‚¡ç¥¨APIè·å–ç§‘æŠ€å…¬å¸è‚¡ä»·
    tech_stocks = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA', 'META']
    stock_info = []

    try:
        # ä½¿ç”¨Yahoo Financeçš„å…è´¹API
        for symbol in tech_stocks[:3]:  # åªè·å–å‰3ä¸ªé¿å…è¯·æ±‚è¿‡å¤š
            try:
                # ç®€å•çš„è‚¡ç¥¨ä¿¡æ¯è·å–
                url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    if 'chart' in data and data['chart']['result']:
                        result = data['chart']['result'][0]
                        meta = result['meta']
                        current_price = meta.get('regularMarketPrice', 0)
                        prev_close = meta.get('previousClose', 0)
                        change = current_price - prev_close if current_price and prev_close else 0
                        change_percent = (change / prev_close * 100) if prev_close else 0

                        stock_info.append({
                            'symbol': symbol,
                            'price': current_price,
                            'change': change,
                            'change_percent': change_percent
                        })

                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"è·å– {symbol} è‚¡ä»·å¤±è´¥: {e}")
                continue

    except Exception as e:
        print(f"è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")

    return stock_info

def get_dev_tools():
    """è·å–å¼€å‘è€…å·¥å…·æ¨è"""
    tools = [
        {
            'name': 'GitHub Copilot',
            'description': 'AIä»£ç åŠ©æ‰‹ï¼Œæé«˜ç¼–ç¨‹æ•ˆç‡',
            'category': 'AIå·¥å…·',
            'url': 'https://github.com/features/copilot'
        },
        {
            'name': 'Postman',
            'description': 'APIå¼€å‘å’Œæµ‹è¯•å¹³å°',
            'category': 'APIå·¥å…·',
            'url': 'https://www.postman.com/'
        },
        {
            'name': 'Figma',
            'description': 'åä½œå¼ç•Œé¢è®¾è®¡å·¥å…·',
            'category': 'è®¾è®¡å·¥å…·',
            'url': 'https://www.figma.com/'
        },
        {
            'name': 'Docker',
            'description': 'å®¹å™¨åŒ–åº”ç”¨éƒ¨ç½²å¹³å°',
            'category': 'éƒ¨ç½²å·¥å…·',
            'url': 'https://www.docker.com/'
        },
        {
            'name': 'VS Code',
            'description': 'è½»é‡çº§ä»£ç ç¼–è¾‘å™¨',
            'category': 'ç¼–è¾‘å™¨',
            'url': 'https://code.visualstudio.com/'
        },
        {
            'name': 'Notion',
            'description': 'å…¨èƒ½å·¥ä½œç©ºé—´å’Œç¬”è®°å·¥å…·',
            'category': 'æ•ˆç‡å·¥å…·',
            'url': 'https://www.notion.so/'
        }
    ]

    # éšæœºé€‰æ‹©2-3ä¸ªå·¥å…·
    selected_tools = random.sample(tools, min(3, len(tools)))
    return selected_tools

def get_coding_challenge():
    """è·å–ç¼–ç¨‹æŒ‘æˆ˜é¢˜ç›®"""
    challenges = [
        {
            'title': 'ä¸¤æ•°ä¹‹å’Œ',
            'difficulty': 'ç®€å•',
            'description': 'ç»™å®šä¸€ä¸ªæ•´æ•°æ•°ç»„å’Œç›®æ ‡å€¼ï¼Œæ‰¾å‡ºæ•°ç»„ä¸­å’Œä¸ºç›®æ ‡å€¼çš„ä¸¤ä¸ªæ•°çš„ç´¢å¼•',
            'tags': ['æ•°ç»„', 'å“ˆå¸Œè¡¨']
        },
        {
            'title': 'æœ€é•¿å›æ–‡å­ä¸²',
            'difficulty': 'ä¸­ç­‰',
            'description': 'ç»™å®šå­—ç¬¦ä¸²ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€é•¿çš„å›æ–‡å­ä¸²',
            'tags': ['å­—ç¬¦ä¸²', 'åŠ¨æ€è§„åˆ’']
        },
        {
            'title': 'äºŒå‰æ ‘çš„æœ€å¤§æ·±åº¦',
            'difficulty': 'ç®€å•',
            'description': 'ç»™å®šäºŒå‰æ ‘ï¼Œæ‰¾å‡ºå…¶æœ€å¤§æ·±åº¦',
            'tags': ['æ ‘', 'é€’å½’']
        },
        {
            'title': 'åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨',
            'difficulty': 'ç®€å•',
            'description': 'å°†ä¸¤ä¸ªå‡åºé“¾è¡¨åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„å‡åºé“¾è¡¨',
            'tags': ['é“¾è¡¨', 'é€’å½’']
        },
        {
            'title': 'æœ‰æ•ˆçš„æ‹¬å·',
            'difficulty': 'ç®€å•',
            'description': 'åˆ¤æ–­å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·æ˜¯å¦æœ‰æ•ˆåŒ¹é…',
            'tags': ['æ ˆ', 'å­—ç¬¦ä¸²']
        }
    ]

    return random.choice(challenges)

def get_mobile_dev_news():
    """è·å–ç§»åŠ¨å¼€å‘ç›¸å…³æ–°é—»"""
    mobile_news = []

    # ç§»åŠ¨å¼€å‘ç›¸å…³RSSæº
    rss_feeds = [
        "https://rsshub.app/juejin/category/android",
        "https://rsshub.app/juejin/category/ios",
        "https://developer.android.com/feeds/all.atom.xml"
    ]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for feed_url in rss_feeds[:1]:  # åªè·å–ä¸€ä¸ªæºé¿å…è¯·æ±‚è¿‡å¤š
        try:
            response = requests.get(feed_url, headers=headers, timeout=15)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)

                for entry in feed.entries[:2]:  # æ¯ä¸ªæºå–2æ¡
                    title = entry.title
                    url = entry.link if hasattr(entry, 'link') else '#'

                    # è·å–æè¿°
                    if hasattr(entry, 'summary'):
                        soup = BeautifulSoup(entry.summary, 'html.parser')
                        description = soup.get_text()[:100] + "..."
                    else:
                        description = "ç§»åŠ¨å¼€å‘ç›¸å…³èµ„è®¯"

                    mobile_news.append({
                        'title': title,
                        'url': url,
                        'description': description
                    })

                    if len(mobile_news) >= 2:
                        break

        except Exception as e:
            print(f"è·å–ç§»åŠ¨å¼€å‘æ–°é—»å¤±è´¥: {e}")
            continue

    # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œæä¾›é»˜è®¤å†…å®¹
    if not mobile_news:
        mobile_news = [
            {
                'title': 'Flutter 3.0 å‘å¸ƒé‡å¤§æ›´æ–°',
                'url': 'https://flutter.dev/',
                'description': 'Googleå‘å¸ƒFlutter 3.0ï¼Œå¸¦æ¥æ›´å¥½çš„æ€§èƒ½å’Œæ–°ç‰¹æ€§'
            },
            {
                'title': 'iOS 17 å¼€å‘è€…é¢„è§ˆç‰ˆå‘å¸ƒ',
                'url': 'https://developer.apple.com/',
                'description': 'è‹¹æœå‘å¸ƒiOS 17å¼€å‘è€…é¢„è§ˆç‰ˆï¼ŒåŒ…å«å¤šé¡¹æ–°åŠŸèƒ½'
            }
        ]

    return mobile_news

def get_tech_trivia():
    """è·å–æŠ€æœ¯è¶£é—»"""
    trivia_list = [
        "ç¬¬ä¸€ä¸ªè®¡ç®—æœºbugæ˜¯ç”±ä¸€åªçœŸæ­£çš„è™«å­å¼•èµ·çš„ - 1947å¹´Grace Hopperåœ¨Harvard Mark IIè®¡ç®—æœºä¸­å‘ç°äº†ä¸€åªé£è›¾",
        "JavaScriptæœ€åˆåªç”¨äº†10å¤©æ—¶é—´å°±è¢«åˆ›é€ å‡ºæ¥ï¼Œç”±Brendan Eichåœ¨1995å¹´å®Œæˆ",
        "ç¬¬ä¸€ä¸ªç½‘ç«™è‡³ä»Šä»åœ¨è¿è¡Œï¼šhttp://info.cern.ch/hypertext/WWW/TheProject.html",
        "Pythonè¯­è¨€çš„åå­—æ¥æºäºè‹±å›½å–œå‰§å›¢ä½“Monty Pythonï¼Œè€Œä¸æ˜¯èŸ’è›‡",
        "ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªåŸŸåæ˜¯symbolics.comï¼Œæ³¨å†Œäº1985å¹´3æœˆ15æ—¥",
        "Linuxä¼é¹…å‰ç¥¥ç‰©Tuxçš„åå­—æ¥æºäºTorvalds UniXçš„ç¼©å†™",
        "ç¬¬ä¸€ä¸ªè®¡ç®—æœºç—…æ¯’å«åšCreeperï¼Œåˆ›å»ºäº1971å¹´ï¼Œå®ƒä¼šæ˜¾ç¤º'I'm the creeper, catch me if you can!'",
        "WiFiè¿™ä¸ªåå­—å®é™…ä¸Šä¸ä»£è¡¨ä»»ä½•ä¸œè¥¿ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªæœ—æœ—ä¸Šå£çš„å“ç‰Œåç§°"
    ]

    return random.choice(trivia_list)

def get_tech_books():
    """è·å–æŠ€æœ¯ä¹¦ç±æ¨è"""
    books = [
        {
            'title': 'ä»£ç æ•´æ´ä¹‹é“',
            'author': 'Robert C. Martin',
            'description': 'ç¼–å†™å¯è¯»ã€å¯ç»´æŠ¤ä»£ç çš„å®è·µæŒ‡å—',
            'category': 'è½¯ä»¶å·¥ç¨‹'
        },
        {
            'title': 'æ·±åº¦å­¦ä¹ ',
            'author': 'Ian Goodfellow',
            'description': 'æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æƒå¨æ•™æ',
            'category': 'äººå·¥æ™ºèƒ½'
        },
        {
            'title': 'è®¾è®¡æ¨¡å¼',
            'author': 'Gang of Four',
            'description': 'é¢å‘å¯¹è±¡è®¾è®¡çš„ç»å…¸æ¨¡å¼',
            'category': 'è½¯ä»¶è®¾è®¡'
        },
        {
            'title': 'Kubernetesæƒå¨æŒ‡å—',
            'author': 'é¾šæ­£ç­‰',
            'description': 'å®¹å™¨ç¼–æ’å¹³å°çš„å®Œæ•´æŒ‡å—',
            'category': 'äº‘åŸç”Ÿ'
        },
        {
            'title': 'Pythonç¼–ç¨‹ï¼šä»å…¥é—¨åˆ°å®è·µ',
            'author': 'Eric Matthes',
            'description': 'Pythonå­¦ä¹ çš„æœ€ä½³å…¥é—¨ä¹¦ç±',
            'category': 'ç¼–ç¨‹è¯­è¨€'
        }
    ]

    return random.choice(books)

def get_cloud_pricing():
    """è·å–äº‘æœåŠ¡ä»·æ ¼ä¿¡æ¯"""
    # æ¨¡æ‹Ÿäº‘æœåŠ¡ä»·æ ¼æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥è°ƒç”¨å„äº‘æœåŠ¡å•†APIï¼‰
    cloud_services = [
        {
            'provider': 'AWS',
            'service': 'EC2 t3.micro',
            'price': '$0.0104/hour',
            'region': 'us-east-1',
            'change': 'æ— å˜åŒ–'
        },
        {
            'provider': 'Azure',
            'service': 'B1S Virtual Machine',
            'price': '$0.0104/hour',
            'region': 'East US',
            'change': 'æ— å˜åŒ–'
        },
        {
            'provider': 'Google Cloud',
            'service': 'e2-micro',
            'price': '$0.0084/hour',
            'region': 'us-central1',
            'change': 'æ— å˜åŒ–'
        }
    ]

    return cloud_services

def get_tech_conferences():
    """è·å–æŠ€æœ¯ä¼šè®®ä¿¡æ¯"""
    # æ¨¡æ‹ŸæŠ€æœ¯ä¼šè®®æ•°æ®
    conferences = [
        {
            'name': 'Google I/O 2025',
            'date': '2025-05-14',
            'location': 'Mountain View, CA',
            'type': 'å¼€å‘è€…å¤§ä¼š',
            'topics': ['AI', 'Android', 'Web']
        },
        {
            'name': 'Apple WWDC 2025',
            'date': '2025-06-05',
            'location': 'San Jose, CA',
            'type': 'å¼€å‘è€…å¤§ä¼š',
            'topics': ['iOS', 'macOS', 'AI']
        },
        {
            'name': 'Microsoft Build 2025',
            'date': '2025-05-21',
            'location': 'Seattle, WA',
            'type': 'å¼€å‘è€…å¤§ä¼š',
            'topics': ['Azure', 'AI', '.NET']
        }
    ]

    # è¿”å›å³å°†ä¸¾è¡Œçš„ä¼šè®®
    today = datetime.now()
    upcoming = []

    for conf in conferences:
        conf_date = datetime.strptime(conf['date'], '%Y-%m-%d')
        if conf_date > today:
            days_until = (conf_date - today).days
            conf['days_until'] = days_until
            upcoming.append(conf)

    return sorted(upcoming, key=lambda x: x['days_until'])[:3]

def get_programming_music():
    """è·å–ç¼–ç¨‹éŸ³ä¹æ¨è"""
    music_recommendations = [
        {
            'title': 'Lofi Hip Hop Radio',
            'artist': 'ChilledCow',
            'genre': 'Lo-fi',
            'description': 'é€‚åˆä¸“æ³¨ç¼–ç¨‹çš„è½»æ¾èƒŒæ™¯éŸ³ä¹'
        },
        {
            'title': 'Brain.fm Focus',
            'artist': 'Brain.fm',
            'genre': 'ä¸“æ³¨éŸ³ä¹',
            'description': 'ç§‘å­¦è®¾è®¡çš„ä¸“æ³¨åŠ›æå‡éŸ³ä¹'
        },
        {
            'title': 'Synthwave Mix',
            'artist': 'Various Artists',
            'genre': 'Synthwave',
            'description': 'å¤å¤æœªæ¥ä¸»ä¹‰ç”µå­éŸ³ä¹ï¼Œæ¿€å‘åˆ›é€ åŠ›'
        },
        {
            'title': 'Ambient Coding',
            'artist': 'Various Artists',
            'genre': 'Ambient',
            'description': 'ç¯å¢ƒéŸ³ä¹ï¼Œè¥é€ å¹³é™çš„ç¼–ç¨‹æ°›å›´'
        }
    ]

    return random.choice(music_recommendations)

def get_startup_news():
    """è·å–åˆ›ä¸šå…¬å¸åŠ¨æ€"""
    # æ¨¡æ‹Ÿåˆ›ä¸šå…¬å¸æ–°é—»
    startup_news = [
        {
            'company': 'OpenAI',
            'news': 'å®Œæˆæ–°ä¸€è½®èèµ„',
            'amount': '$10B',
            'description': 'ä¼°å€¼è¾¾åˆ°$80Bï¼Œç»§ç»­é¢†è·‘AIé¢†åŸŸ'
        },
        {
            'company': 'Anthropic',
            'news': 'å‘å¸ƒClaude 3.5',
            'amount': 'N/A',
            'description': 'åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠGPT-4'
        },
        {
            'company': 'Mistral AI',
            'news': 'æ¨å‡ºå¼€æºå¤§æ¨¡å‹',
            'amount': 'N/A',
            'description': 'æŒ‘æˆ˜OpenAIçš„å¸‚åœºåœ°ä½'
        }
    ]

    return random.sample(startup_news, min(2, len(startup_news)))

def get_design_resources():
    """è·å–è®¾è®¡èµ„æºæ¨è"""
    design_resources = [
        {
            'name': 'Dribbble',
            'type': 'è®¾è®¡çµæ„Ÿ',
            'url': 'https://dribbble.com/',
            'description': 'å…¨çƒè®¾è®¡å¸ˆä½œå“å±•ç¤ºå¹³å°'
        },
        {
            'name': 'Unsplash',
            'type': 'å…è´¹å›¾ç‰‡',
            'url': 'https://unsplash.com/',
            'description': 'é«˜è´¨é‡å…è´¹å›¾ç‰‡èµ„æº'
        },
        {
            'name': 'Coolors',
            'type': 'é…è‰²å·¥å…·',
            'url': 'https://coolors.co/',
            'description': 'æ™ºèƒ½é…è‰²æ–¹æ¡ˆç”Ÿæˆå™¨'
        },
        {
            'name': 'Figma Community',
            'type': 'è®¾è®¡æ¨¡æ¿',
            'url': 'https://www.figma.com/community/',
            'description': 'å…è´¹è®¾è®¡æ¨¡æ¿å’Œç»„ä»¶åº“'
        }
    ]

    return random.sample(design_resources, min(2, len(design_resources)))

def get_learning_path():
    """è·å–æŠ€èƒ½å­¦ä¹ è·¯å¾„æ¨è"""
    learning_paths = [
        {
            'skill': 'å…¨æ ˆå¼€å‘',
            'level': 'åˆçº§åˆ°ä¸­çº§',
            'duration': '6-12ä¸ªæœˆ',
            'steps': ['HTML/CSSåŸºç¡€', 'JavaScript', 'React/Vue', 'Node.js', 'æ•°æ®åº“', 'éƒ¨ç½²']
        },
        {
            'skill': 'AI/æœºå™¨å­¦ä¹ ',
            'level': 'ä¸­çº§',
            'duration': '8-15ä¸ªæœˆ',
            'steps': ['PythonåŸºç¡€', 'æ•°å­¦åŸºç¡€', 'TensorFlow/PyTorch', 'æ·±åº¦å­¦ä¹ ', 'é¡¹ç›®å®è·µ']
        },
        {
            'skill': 'äº‘åŸç”Ÿå¼€å‘',
            'level': 'ä¸­çº§åˆ°é«˜çº§',
            'duration': '4-8ä¸ªæœˆ',
            'steps': ['Docker', 'Kubernetes', 'å¾®æœåŠ¡', 'DevOps', 'ç›‘æ§è¿ç»´']
        },
        {
            'skill': 'ç½‘ç»œå®‰å…¨',
            'level': 'åˆçº§åˆ°ä¸­çº§',
            'duration': '6-10ä¸ªæœˆ',
            'steps': ['ç½‘ç»œåŸºç¡€', 'ç³»ç»Ÿå®‰å…¨', 'æ¸—é€æµ‹è¯•', 'å®‰å…¨å·¥å…·', 'åˆè§„è®¤è¯']
        }
    ]

    return random.choice(learning_paths)

def get_tech_trends():
    """è·å–æŠ€æœ¯çƒ­è¯è¶‹åŠ¿"""
    # æ¨¡æ‹ŸæŠ€æœ¯çƒ­è¯æ•°æ®
    tech_keywords = [
        {'keyword': 'AI', 'trend': 'ğŸ”¥', 'change': '+15%', 'description': 'äººå·¥æ™ºèƒ½æŒç»­ç«çƒ­'},
        {'keyword': 'Kubernetes', 'trend': 'ğŸ“ˆ', 'change': '+8%', 'description': 'å®¹å™¨ç¼–æ’éœ€æ±‚å¢é•¿'},
        {'keyword': 'Rust', 'trend': 'ğŸš€', 'change': '+12%', 'description': 'ç³»ç»Ÿç¼–ç¨‹è¯­è¨€å´›èµ·'},
        {'keyword': 'WebAssembly', 'trend': 'â­', 'change': '+6%', 'description': 'Webæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯'},
        {'keyword': 'Edge Computing', 'trend': 'ğŸ“Š', 'change': '+10%', 'description': 'è¾¹ç¼˜è®¡ç®—åº”ç”¨æ‰©å±•'},
        {'keyword': 'Quantum Computing', 'trend': 'ğŸ”¬', 'change': '+4%', 'description': 'é‡å­è®¡ç®—ç ”ç©¶è¿›å±•'},
        {'keyword': 'Blockchain', 'trend': 'ğŸ“‰', 'change': '-3%', 'description': 'åŒºå—é“¾çƒ­åº¦å›è½'},
        {'keyword': 'Serverless', 'trend': 'â˜ï¸', 'change': '+7%', 'description': 'æ— æœåŠ¡å™¨æ¶æ„æ™®åŠ'}
    ]

    # éšæœºé€‰æ‹©5ä¸ªçƒ­è¯
    selected_trends = random.sample(tech_keywords, min(5, len(tech_keywords)))
    return sorted(selected_trends, key=lambda x: int(x['change'].replace('%', '').replace('+', '').replace('-', '')), reverse=True)

def get_github_stats():
    """è·å–GitHubç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–ä¸€äº›æœ‰è¶£çš„GitHubç»Ÿè®¡
        stats = {
            'total_repos': '100M+',
            'active_developers': '73M+',
            'daily_commits': '1M+',
            'languages_used': '500+',
            'open_source_projects': '28M+'
        }
        return stats
    except Exception as e:
        print(f"è·å–GitHubç»Ÿè®¡å¤±è´¥: {e}")
        return None

def get_developer_joke():
    """è·å–ç¨‹åºå‘˜ç¬‘è¯"""
    jokes = [
        "ä¸ºä»€ä¹ˆç¨‹åºå‘˜å–œæ¬¢é»‘æš—ï¼Ÿå› ä¸ºå…‰ä¼šäº§ç”Ÿbugï¼",
        "ç¨‹åºå‘˜çš„ä¸‰å¤§ç¾å¾·ï¼šæ‡’æƒ°ã€æ€¥èºå’Œå‚²æ…¢ã€‚",
        "ä¸–ç•Œä¸Šæœ‰10ç§äººï¼šæ‡‚äºŒè¿›åˆ¶çš„å’Œä¸æ‡‚äºŒè¿›åˆ¶çš„ã€‚",
        "è°ƒè¯•å°±åƒæ˜¯çŠ¯ç½ªç”µå½±ä¸­çš„ä¾¦æ¢ï¼Œä½ æ—¢æ˜¯ä¾¦æ¢ï¼Œä¹Ÿæ˜¯å‡¶æ‰‹ã€‚",
        "ç¨‹åºå‘˜æœ€è®¨åŒçš„ä¸¤ä»¶äº‹ï¼š1. å†™æ–‡æ¡£ 2. æ²¡æœ‰æ–‡æ¡£",
        "å¦‚æœè°ƒè¯•æ˜¯å»é™¤bugçš„è¿‡ç¨‹ï¼Œé‚£ä¹ˆç¼–ç¨‹å°±æ˜¯æ”¾å…¥bugçš„è¿‡ç¨‹ã€‚",
        "çœŸæ­£çš„ç¨‹åºå‘˜ä¸éœ€è¦æ³¨é‡Šï¼Œä»£ç å°±æ˜¯æœ€å¥½çš„æ–‡æ¡£ã€‚",
        "ç¨‹åºå‘˜çš„å£å¤´ç¦…ï¼šåœ¨æˆ‘çš„æœºå™¨ä¸Šè¿è¡Œå¾—å¾ˆå¥½ï¼"
    ]

    return random.choice(jokes)

def get_tech_salary_info():
    """è·å–æŠ€æœ¯è–ªèµ„ä¿¡æ¯"""
    salary_data = [
        {
            'position': 'AIå·¥ç¨‹å¸ˆ',
            'level': 'ä¸­çº§',
            'salary_range': '25-40ä¸‡',
            'trend': 'ğŸ“ˆ',
            'hot_skills': ['Python', 'TensorFlow', 'PyTorch']
        },
        {
            'position': 'å…¨æ ˆå¼€å‘',
            'level': 'ä¸­çº§',
            'salary_range': '20-35ä¸‡',
            'trend': 'ğŸ“Š',
            'hot_skills': ['React', 'Node.js', 'TypeScript']
        },
        {
            'position': 'äº‘æ¶æ„å¸ˆ',
            'level': 'é«˜çº§',
            'salary_range': '35-60ä¸‡',
            'trend': 'ğŸš€',
            'hot_skills': ['AWS', 'Kubernetes', 'DevOps']
        },
        {
            'position': 'å®‰å…¨å·¥ç¨‹å¸ˆ',
            'level': 'ä¸­çº§',
            'salary_range': '22-38ä¸‡',
            'trend': 'ğŸ“ˆ',
            'hot_skills': ['æ¸—é€æµ‹è¯•', 'å®‰å…¨å®¡è®¡', 'Python']
        }
    ]

    return random.choice(salary_data)

def get_open_source_spotlight():
    """è·å–å¼€æºé¡¹ç›®èšç„¦"""
    projects = [
        {
            'name': 'Tauri',
            'description': 'ä½¿ç”¨Rustæ„å»ºè·¨å¹³å°æ¡Œé¢åº”ç”¨',
            'language': 'Rust',
            'stars': '70k+',
            'why_interesting': 'æ¯”Electronæ›´è½»é‡çš„æ¡Œé¢åº”ç”¨è§£å†³æ–¹æ¡ˆ'
        },
        {
            'name': 'SvelteKit',
            'description': 'ç°ä»£Webåº”ç”¨æ¡†æ¶',
            'language': 'JavaScript',
            'stars': '15k+',
            'why_interesting': 'ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼Œè¿è¡Œæ—¶æ€§èƒ½ä¼˜å¼‚'
        },
        {
            'name': 'Deno',
            'description': 'ç°ä»£JavaScript/TypeScriptè¿è¡Œæ—¶',
            'language': 'Rust/TypeScript',
            'stars': '90k+',
            'why_interesting': 'Node.jsåˆ›å§‹äººçš„æ–°ä½œå“ï¼Œå†…ç½®TypeScriptæ”¯æŒ'
        },
        {
            'name': 'Zed',
            'description': 'é«˜æ€§èƒ½ä»£ç ç¼–è¾‘å™¨',
            'language': 'Rust',
            'stars': '25k+',
            'why_interesting': 'ä¸“ä¸ºåä½œç¼–ç¨‹è®¾è®¡çš„ç°ä»£ç¼–è¾‘å™¨'
        }
    ]

    return random.choice(projects)

if __name__ == "__main__":
    update_readme()